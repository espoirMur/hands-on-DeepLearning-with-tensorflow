{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is notebook is about the first assignement from the udacity deep learning course the assigment is about  MNIST handwritten recogniton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first of all let import the usefull libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "import imageio\n",
    "# Config the matplotlib backend as plotting inline in IPython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "First, we'll download the dataset to our local machine. The data consists of characters rendered in a variety of fonts on a 28x28 image. The labels are limited to 'A' through 'J' (10 classes). The training set has about 500k and the testset 19000 labeled examples. Given these sizes, it should be possible to train models quickly on any machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to download: notMNIST_large.tar.gz\n",
      "0%....5%....10%....15%....20%....25%....30%....35%....40%....45%....50%....55%....60%....65%....70%....75%....80%....85%....90%....95%....100%\n",
      "Download Complete!\n",
      "Found and verified ../datasets/notMNIST_large.tar.gz\n",
      "Attempting to download: notMNIST_small.tar.gz\n",
      "0%....5%....10%....15%....20%....25%....30%....35%....40%....45%....50%....55%....60%....65%....70%....75%....80%....85%....90%....95%....100%\n",
      "Download Complete!\n",
      "Found and verified ../datasets/notMNIST_small.tar.gz\n"
     ]
    }
   ],
   "source": [
    "url = 'https://commondatastorage.googleapis.com/books1000/'\n",
    "last_percent_reported = None\n",
    "data_root = '../datasets' \n",
    "\n",
    "def download_progress_hook(count, blockSize, totalSize):\n",
    "  \"\"\"\n",
    "  A hook to report the progress of a download. This is mostly intended for users with\n",
    "  slow internet connections. Reports every 5% change in download progress.\n",
    "  \n",
    "  \"\"\"\n",
    "  global last_percent_reported\n",
    "  percent = int(count * blockSize * 100 / totalSize)\n",
    "\n",
    "  if last_percent_reported != percent:\n",
    "    if percent % 5 == 0:\n",
    "      sys.stdout.write(\"%s%%\" % percent)\n",
    "      sys.stdout.flush()\n",
    "    else:\n",
    "      sys.stdout.write(\".\")\n",
    "      sys.stdout.flush()\n",
    "      \n",
    "    last_percent_reported = percent\n",
    "    \n",
    "def maybe_download(filename, expected_bytes, force=False):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  dest_filename = os.path.join(data_root, filename)\n",
    "  if force or not os.path.exists(dest_filename):\n",
    "    print('Attempting to download:', filename) \n",
    "    filename, _ = urlretrieve(url + filename, dest_filename, reporthook=download_progress_hook)\n",
    "    print('\\nDownload Complete!')\n",
    "  statinfo = os.stat(dest_filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified', dest_filename)\n",
    "  else:\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + dest_filename + '. Can you get to it with a browser?')\n",
    "  return dest_filename\n",
    "\n",
    "train_filename = maybe_download('notMNIST_large.tar.gz', 247336696)\n",
    "test_filename = maybe_download('notMNIST_small.tar.gz', 8458043)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the file downloaded We need to extract the dataset : \n",
    "    here is the code for that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data for ../datasets/notMNIST_large . This may take a while. Please wait.\n",
      "['../datasets/notMNIST_large/A', '../datasets/notMNIST_large/B', '../datasets/notMNIST_large/C', '../datasets/notMNIST_large/D', '../datasets/notMNIST_large/E', '../datasets/notMNIST_large/F', '../datasets/notMNIST_large/G', '../datasets/notMNIST_large/H', '../datasets/notMNIST_large/I', '../datasets/notMNIST_large/J']\n",
      "Extracting data for ../datasets/notMNIST_small . This may take a while. Please wait.\n",
      "['../datasets/notMNIST_small/A', '../datasets/notMNIST_small/B', '../datasets/notMNIST_small/C', '../datasets/notMNIST_small/D', '../datasets/notMNIST_small/E', '../datasets/notMNIST_small/F', '../datasets/notMNIST_small/G', '../datasets/notMNIST_small/H', '../datasets/notMNIST_small/I', '../datasets/notMNIST_small/J']\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "np.random.seed(133)\n",
    "\n",
    "def maybe_extract(filename, force=False):\n",
    "  root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
    "  if os.path.isdir(root) and not force:\n",
    "    # You may override by setting force=True.\n",
    "    print('{} already present - Skipping extraction of {}.'.format(root, filename))\n",
    "  else:\n",
    "    print('Extracting data for {} . This may take a while. Please wait.'.format(root))\n",
    "    tar = tarfile.open(filename)\n",
    "    sys.stdout.flush()\n",
    "    tar.extractall(data_root)\n",
    "    tar.close()\n",
    "  data_folders = [\n",
    "    os.path.join(root, d) for d in sorted(os.listdir(root))\n",
    "    if os.path.isdir(os.path.join(root, d))]\n",
    "  if len(data_folders) != num_classes:\n",
    "    raise Exception(\n",
    "      'Expected %d folders, one per class. Found %d instead.' % (\n",
    "        num_classes, len(data_folders)))\n",
    "  print(data_folders)\n",
    "  return data_folders\n",
    "  \n",
    "train_folders = maybe_extract(train_filename)\n",
    "test_folders = maybe_extract(test_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sample_image = Image(filename='../datasets/notMNIST_large/A/em9ya29uLnR0Zg==.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAB50lEQVR4nG2SO2iUURCFz8y9uyQx\nENEtIqbQxUKiQUUrlUDsFMQHpAyCQRtDGgkidgELC1FsrGULi4CFCCoWqQQfhVpIEFGJCfGRFcVk\nY3b/O3MsVvcRPTAwzOGcW9wPaCoX0S5trt39+XwMIfzHVBnoX6klM5OG2djUHy5/LCzXdPqO8J/6\nobSQSDLbKto4/mmQkdCryawaz1Dbg4L1L5jcvVbzuU6RNlNRcidJY8YJhFYv4ASN7rPf6e6VLdJS\nLNox40bj4FVmzHi9NRowxsTEp9hRdXf/0deMivTMuzHxmGCKGTPeaEYjxpmY/GVUDJrRfHHj36hI\nx1s3Jp5CzGGKKVvmFQQgAIgYYaL5hy5RYGfVmXypFwpRiMfzBFxurgQfOLTpk3jw7jEREAg4yUT3\nckF0W4V0c0/+CgIoGC8CcLlVzvvxzqqLiqg86qm/eJiJbr+KooU5Mn15/e5rpXR5FAHquQsEXCff\n53yi79rzMP7kwVJ53bP5IkVw9K4F0/vDq8TBbzPFI7f37t51YPbz4p79CnmcalblOcTmP8V99+gc\nBk6TJBcKIoCqaJCIzZdGf5JvuuL26c4OxFI5GOAAAUM6u6Gyaj1DUueAwuBqjeJ8TpMDIm0QtiEp\n9VkDY91dS2i7fgPBVP+0loxjkQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let Now load the images in a managable format the images will be loaded in a 3D array\n",
    "the shape of the array will be (number of training example , 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_size = 28  # Pixel width and height.\n",
    "pixel_depth = 255.0  # Number of levels per pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_letter(folder, min_num_images):\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Load the data for a single letter label.\n",
    "    \"\"\"\n",
    "    image_files = os.listdir(folder) #list all the files in the directory of an given image\n",
    "    dataset = np.ndarray(shape=(len(image_files), image_size, image_size),\n",
    "                         dtype=np.float32)\n",
    "    print(folder)\n",
    "    num_images = 0\n",
    "    for image in image_files:\n",
    "        image_file = os.path.join(folder, image)\n",
    "        try:\n",
    "            #reading the image and convert it to a matrix of each pixel and normalize them \n",
    "            image_data = (imageio.imread(image_file).astype(float) - pixel_depth / 2) / pixel_depth\n",
    "            if image_data.shape != (image_size, image_size):\n",
    "                raise Exception('Unexpected image shape: {}'.format(str(image_data.shape)))\n",
    "            dataset[num_images, :, :] = image_data #adding the image to the dataset\n",
    "            num_images +=1\n",
    "        except (IOError, ValueError) as e:\n",
    "            print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "    dataset = dataset[0:num_images, :, :]\n",
    "    if num_images < min_num_images:\n",
    "        raise Exception('Many fewer images than expected: {} < {}'.format(num_images, min_num_images))\n",
    "    print('Full dataset tensor:', dataset.shape)\n",
    "    print('Mean:', np.mean(dataset))\n",
    "    print('Standard deviation:', np.std(dataset))\n",
    "    return dataset\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../datasets/notMNIST_large/A/\n",
      "Could not read: ../datasets/notMNIST_large/A/RnJlaWdodERpc3BCb29rSXRhbGljLnR0Zg==.png : Could not find a format to read the specified file in mode 'i' - it's ok, skipping.\n",
      "Could not read: ../datasets/notMNIST_large/A/SG90IE11c3RhcmQgQlROIFBvc3Rlci50dGY=.png : Could not find a format to read the specified file in mode 'i' - it's ok, skipping.\n",
      "Could not read: ../datasets/notMNIST_large/A/Um9tYW5hIEJvbGQucGZi.png : Could not find a format to read the specified file in mode 'i' - it's ok, skipping.\n",
      "Full dataset tensor: (52909, 28, 28)\n",
      "Mean: -0.12825\n",
      "Standard deviation: 0.443121\n"
     ]
    }
   ],
   "source": [
    "dataset_a = load_letter('../datasets/notMNIST_large/A/',1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_pickle(data_folders, min_num_images_per_class, force=False):\n",
    "    \"\"\"\n",
    "    pickle some some training example from images dataset\n",
    "    \"\"\"\n",
    "    dataset_names = []\n",
    "    for folder in data_folders:\n",
    "        set_filename = folder + '.pickle'\n",
    "        dataset_names.append(set_filename)\n",
    "    if os.path.exists(set_filename) and not force:\n",
    "        # You may override by setting force=True.\n",
    "        print('{} already present - Skipping pickling.'.format(set_filename))\n",
    "    else:\n",
    "        print('Pickling {}.'.format(set_filename))\n",
    "        dataset = load_letter(folder, min_num_images_per_class)\n",
    "        try:\n",
    "            with open(set_filename, 'wb') as f:\n",
    "                pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "        except Exception as e:\n",
    "            print('Unable to save data to', set_filename, ':', e)\n",
    "    return dataset_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tarinfloa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_folders' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-e5ade8b6b828>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_datasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_folders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m45000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_datasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_folders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1800\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_folders' is not defined"
     ]
    }
   ],
   "source": [
    "train_datasets = maybe_pickle(train_folders, 45000)\n",
    "test_datasets = maybe_pickle(test_folders, 1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
