{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started With Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this notebook is for my exercices related to tensorflow from the book : hand on on a Machine Learning Project with  sklearn and tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let create a graph for the function f(x, y) = x^2*y + y + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(3, name='x')\n",
    "y = tf.Variable(4, name='y')\n",
    "f = x*x*y + y + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then run the graph and initialize the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session():\n",
    "    x.initializer.run()\n",
    "    y.initializer.run()\n",
    "    result = f.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see the final result as the output of our program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but instead of initializing global variables manually we can use global_variable_initializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let do it for our graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    result = f.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tensorflow program is typycally split in 2 parts :\n",
    "    - computation phase (graph construction )\n",
    "    - execution phase ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using tensor flow to compute a linear regression on the boston housing prediction dataset see in chapiter 4 of the book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen in the previous part that a tensor operations can take one input (a number : x , and y) but in general a tensor \n",
    "can take an input of any size and return an output of any size (like numpy ndarray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let applied those tensor operations to california housing price dataset which is a 2D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /Users/espyMur/scikit_learn_data\n",
      "INFO:sklearn.datasets.california_housing:Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /Users/espyMur/scikit_learn_data\n"
     ]
    }
   ],
   "source": [
    "housing = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = housing.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "housing_with_biais =np.c_[np.ones((m,1)), housing.data] #adding the biais term to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.00000000e+00,   8.32520000e+00,   4.10000000e+01,\n",
       "          6.98412698e+00,   1.02380952e+00,   3.22000000e+02,\n",
       "          2.55555556e+00,   3.78800000e+01,  -1.22230000e+02],\n",
       "       [  1.00000000e+00,   8.30140000e+00,   2.10000000e+01,\n",
       "          6.23813708e+00,   9.71880492e-01,   2.40100000e+03,\n",
       "          2.10984183e+00,   3.78600000e+01,  -1.22220000e+02],\n",
       "       [  1.00000000e+00,   7.25740000e+00,   5.20000000e+01,\n",
       "          8.28813559e+00,   1.07344633e+00,   4.96000000e+02,\n",
       "          2.80225989e+00,   3.78500000e+01,  -1.22240000e+02],\n",
       "       [  1.00000000e+00,   5.64310000e+00,   5.20000000e+01,\n",
       "          5.81735160e+00,   1.07305936e+00,   5.58000000e+02,\n",
       "          2.54794521e+00,   3.78500000e+01,  -1.22250000e+02],\n",
       "       [  1.00000000e+00,   3.84620000e+00,   5.20000000e+01,\n",
       "          6.28185328e+00,   1.08108108e+00,   5.65000000e+02,\n",
       "          2.18146718e+00,   3.78500000e+01,  -1.22250000e+02]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_with_biais[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_array = housing_with_biais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "housing_dataframe = pd.DataFrame(data=housing_array, columns=['A0']  + housing.feature_names) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let compute the linear regression which consist on checking thetha that minimaze the cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find more on the following links "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://ayearofai.com/rohan-3-deriving-the-normal-equation-using-matrix-calculus-1a1b16f65dda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://eli.thegreenplace.net/2014/derivation-of-the-normal-equation-for-linear-regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the theata is given by the following equation : the Normal Equation (theta = (XT · X)–1 · XT. y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = tf.constant(value=housing_array, dtype=tf.float32, name=\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = tf.constant(value=housing.target.reshape(-1,1), dtype=tf.float32, name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "XT = tf.transpose(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "theta_values = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = y - tf.matmul(X, theta_values)\n",
    "mean_suqare_error_normal = tf.reduce_mean((tf.square(error)), name='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    theta_values = theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have computed more easily the theta values by using the normal equation let now try to use gradient descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Implementing Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let now compute a step by step gradient descent to find the values of theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "before computing the gradient descent let scale the housing data in order to speed the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing  import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_array_scalled = scaler.fit_transform(housing_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let check if the values are scaled "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean = np.mean(housing_array[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = np.std(housing_array[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_value = housing_array[1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_scaled_value = (a_value - mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3322379635373314"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_scaled_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3322379635373314"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_array_scalled[1,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can notice  that we have standarized  the data and return the data center with zero mean and unit variance by using this formula:\n",
    "x_scaled = (x - mean(x_column))/std(x_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 1000 #number of iterations to use while computting the gradient descent\n",
    "learning_rate = 0.01 #the alpha value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_array_scalled[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_array_scalled[:,0] = np.ones((1,m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.constant(housing_array_scalled, dtype=tf.float32, name=\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = tf.Variable(tf.random_uniform(shape=[n+1,1], minval=-1, maxval=1), name='theta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_predicted = tf.matmul(X, theta, name='predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = y_predicted - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_suqare_error = tf.reduce_mean((tf.square(error)), name='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assign() function creates a node that will assign a new value to a variable. In this case, it\n",
    "implements the Batch Gradient Descent step θ(next step) = θ – η∇θMSE(θ)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "training_ops = tf.assign(theta, theta - learning_rate * gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 mean square error =  9.34639\n",
      "Epoch 100 mean square error =  0.740632\n",
      "Epoch 200 mean square error =  0.548406\n",
      "Epoch 300 mean square error =  0.537094\n",
      "Epoch 400 mean square error =  0.534158\n",
      "Epoch 500 mean square error =  0.5322\n",
      "Epoch 600 mean square error =  0.530675\n",
      "Epoch 700 mean square error =  0.529465\n",
      "Epoch 800 mean square error =  0.528498\n",
      "Epoch 900 mean square error =  0.527723\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epoch):\n",
    "        if epoch % 100 == 0:\n",
    "            print (\"Epoch\", epoch, \"mean square error = \", mean_suqare_error.eval() )\n",
    "        sess.run(training_ops)\n",
    "    best_theta = theta.eval()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous session we have computed the gradient of our cost function manually, but in many case(for example when working with deep neural networks) it will not be easy to derivate the cost function manually that why tensorflow libary has a powerful tool that do it for us it's called autodiff, not also that we can use an optimizer to train our gradient descent algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Saving and Restoring the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it a good practice once a model trained to save it in order to be able to reload it for future use , it can also be wise to save some checkpoint of the training model so that if the computer crashes during the training phase we can reload it and continue the training from the last checkpoint rather than restart all the training ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let add a saver in our example and use it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 mean square error =  11.3479\n",
      "Epoch 100 mean square error =  0.710132\n",
      "Epoch 200 mean square error =  0.544772\n",
      "Epoch 300 mean square error =  0.537151\n",
      "Epoch 400 mean square error =  0.533628\n",
      "Epoch 500 mean square error =  0.531111\n",
      "Epoch 600 mean square error =  0.529283\n",
      "Epoch 700 mean square error =  0.527954\n",
      "Epoch 800 mean square error =  0.526987\n",
      "Epoch 900 mean square error =  0.526281\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epoch):\n",
    "        if epoch % 100 == 0:\n",
    "            print (\"Epoch\", epoch, \"mean square error = \", mean_suqare_error.eval() )\n",
    "            saver.save(sess, '../models/my_model.ckpt')\n",
    "        sess.run(training_ops)\n",
    "    best_theta = theta.eval()\n",
    "    saver.save(sess, '../models/my_model_final.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have saved our model we can reload it in the execution phase , by calling the restore method of the saver in the initialisation phase instead of manually initialise the variable, and the saver will reload all the variables with theirs names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Graph and Training Curves Using TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorflow has a good tool for visualizing training graphs and training statistics likes learning curves , it's called tensorboard it can be runned directly in the browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let tweak our program so that it can write the logs and the metrics we are looking for in a log directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.utcnow().strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "root_logdir = \"tensorflow_logs\"\n",
    "logdir = \"../{}/run-{}/\".format(root_logdir, now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../tensorflow_logs/run-2017-11-30-08:28:16/'"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "mse_summary = tf.summary.scalar('MSE', mean_suqare_error) #the stat to use\n",
    "file_writer = tf.summary.FileWriter(logdir=logdir,graph=tf.get_default_graph()) #initializing the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epoch):\n",
    "        if epoch % 10 == 0:\n",
    "            #print (\"Epoch\", epoch, \"mean square error = \", mean_suqare_error.eval() )\n",
    "            #saver.save(sess, '../models/my_model.ckpt')\n",
    "            \n",
    "            file_writer.add_summary(mse_summary.eval(), epoch)\n",
    "        sess.run(training_ops)\n",
    "    best_theta = theta.eval()\n",
    "    saver.save(sess, '../models/my_model_final.ckpt')\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to visualize the graph for ours operations we need to run tensorboard in the log directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That all for today, next we will try to implement a logistic regression classifier with tensorflow by following the same approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini Batch Gradient Descent With Logistic Regresssion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
